{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "possible-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "later-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsspecial(s):\n",
    "    regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "          \n",
    "    if(regex.search(s) == None):\n",
    "        return True\n",
    "          \n",
    "    else:\n",
    "        return False\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recreational-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "maxlen = 30\n",
    "labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "uniform-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_csv=['Sno','Name','Father_name','Husband_name','Mother_name','House_number','Gender','Age']\n",
    "\n",
    "# cols_csv=['Sno','Name','Gender' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cloudy-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jalend15/opt/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# input = pd.read_csv(\"/Users/jalend15/Downloads/cbse_gender_names_list.csv\" )\n",
    "input = pd.read_csv(\"/Users/jalend15/Downloads/combine/combined.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "premier-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sno          Name  Father_name Husband_name Mother_name House_number  \\\n",
      "0  0.0   TABING TABO  TAMANG TABO          NaN         NaN          E-1   \n",
      "1  1.0   TALUNG TABO  TAMANG TABO          NaN         NaN          E-1   \n",
      "2  2.0    YABUR TABO  TAMANG TABO          NaN         NaN          E-1   \n",
      "3  3.0  YASONG TABOH          NaN   TATE TABOH         NaN          E-1   \n",
      "4  4.0   YAKEN TABOH   TATE TABOH          NaN         NaN          E-1   \n",
      "\n",
      "   Gender Age  \n",
      "0    MALE  42  \n",
      "1    MALE  39  \n",
      "2  FEMALE  37  \n",
      "3  FEMALE  55  \n",
      "4  FEMALE  34  \n",
      "(21163654, 8)\n"
     ]
    }
   ],
   "source": [
    "input.columns = cols_csv\n",
    "print(input.head())\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ambient-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "yellow-indonesia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21163648, 8)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "embedded-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = pd.read_csv(\"gender_data.csv\",header=None)\n",
    "# input.columns = ['name','m_or_f']\n",
    "input['namelen']= [len(str(i)) for i in input['Name']]\n",
    "input['sp']= [containsspecial(str(i)) for i in input['Name']]\n",
    "input1 = input[(input['namelen'] >= 2) ]\n",
    "input1= input1[(input1['sp']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "immune-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1['first_name'] = [str(name).split(' ')[0]  for name in input1['Name'].values.astype('str')]\n",
    "input1['last_name'] = [' '.join(str(name).split(\" \")[1:]) for name in input1['Name'].values.astype('str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "phantom-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = input[(input['namelen']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "electric-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20867689, 1)\n"
     ]
    }
   ],
   "source": [
    "first_names = pd.DataFrame(input1['first_name'])\n",
    "print(first_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "distinct-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names = first_names.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "wrapped-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(first_names)) < 1\n",
    "train_first_names = first_names[msk]\n",
    "test_first_names = first_names[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "suspected-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500768, 1)\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_first_names.shape)\n",
    "print(test_first_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "greek-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name\n",
      "0     TABING\n",
      "1     TALUNG\n",
      "2      YABUR\n",
      "3     YASONG\n",
      "4      YAKEN\n"
     ]
    }
   ],
   "source": [
    "print(train_first_names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "silver-repository",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1['first_name'][0] in train_first_names['first_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cheap-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20867689,)\n"
     ]
    }
   ],
   "source": [
    "msk = np.ones(len(input1), dtype=bool)\n",
    "print(msk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "sitting-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Sno                    Name    Father_name Husband_name  \\\n",
      "0              0.0             TABING TABO    TAMANG TABO          NaN   \n",
      "1              1.0             TALUNG TABO    TAMANG TABO          NaN   \n",
      "2              2.0              YABUR TABO    TAMANG TABO          NaN   \n",
      "3              3.0            YASONG TABOH            NaN   TATE TABOH   \n",
      "4              4.0             YAKEN TABOH     TATE TABOH          NaN   \n",
      "...            ...                     ...            ...          ...   \n",
      "21163647  398929.0            SIMON TAMANG     BHAKTA BDR          NaN   \n",
      "21163648  398930.0         ASHRITA BASNETT  PAWAN BASNETT          NaN   \n",
      "21163649  398931.0        RAJ KUMAR LIMBOO            NaN          NaN   \n",
      "21163650  398932.0            ELINA RAI re            NaN          NaN   \n",
      "21163653  398935.0  Husbands  - JUSEIN RAL            NaN          NaN   \n",
      "\n",
      "         Mother_name       House_number  Gender Age  namelen    sp first_name  \\\n",
      "0                NaN                E-1    MALE  42       11  True     TABING   \n",
      "1                NaN                E-1    MALE  39       11  True     TALUNG   \n",
      "2                NaN                E-1  FEMALE  37       10  True      YABUR   \n",
      "3                NaN                E-1  FEMALE  55       12  True     YASONG   \n",
      "4                NaN                E-1  FEMALE  34       11  True      YAKEN   \n",
      "...              ...                ...     ...  ..      ...   ...        ...   \n",
      "21163647         NaN                NaN    MALE  19       12  True      SIMON   \n",
      "21163648         NaN                NaN    MALE  20       15  True    ASHRITA   \n",
      "21163649         NaN  NCB/02/143(1) con    MALE  21       16  True        RAJ   \n",
      "21163650         NaN                  )  FEMALE  20       12  True      ELINA   \n",
      "21163653         NaN                NIL  FEMALE  37       22  True   Husbands   \n",
      "\n",
      "              last_name  \n",
      "0                  TABO  \n",
      "1                  TABO  \n",
      "2                  TABO  \n",
      "3                 TABOH  \n",
      "4                 TABOH  \n",
      "...                 ...  \n",
      "21163647         TAMANG  \n",
      "21163648        BASNETT  \n",
      "21163649   KUMAR LIMBOO  \n",
      "21163650         RAI re  \n",
      "21163653   - JUSEIN RAL  \n",
      "\n",
      "[20867689 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(input1)\n",
    "input2= input1\n",
    "\n",
    "input1 = input2.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "rising-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(msk)):\n",
    "#   print(i)\n",
    "#   if input1['first_name'][i] in train_first_names['first_name'].tolist():\n",
    "#     msk[i] = True\n",
    "#   else :\n",
    "#     msk[i] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "monetary-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(input1)) < 0.7\n",
    "\n",
    "train = input1[msk]\n",
    "test = input1[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "suitable-mortality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14610390\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "controlling-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=train.sample(700000)\n",
    "# test = test.sample(300000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "interested-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MALE      3287641\n",
       "FEMALE    2918143\n",
       "Name: Gender, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "negative-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Sno                    Name  Father_name Husband_name  \\\n",
      "0              0.0             TABING TABO  TAMANG TABO          NaN   \n",
      "1              1.0             TALUNG TABO  TAMANG TABO          NaN   \n",
      "2              2.0              YABUR TABO  TAMANG TABO          NaN   \n",
      "4              4.0             YAKEN TABOH   TATE TABOH          NaN   \n",
      "5              5.0             YAPET TABOH   TATE TABOH          NaN   \n",
      "...            ...                     ...          ...          ...   \n",
      "20867683  398928.0     DANIAL SHANKER KAMI  SHYAM KUMAR          NaN   \n",
      "20867684  398929.0            SIMON TAMANG   BHAKTA BDR          NaN   \n",
      "20867686  398931.0        RAJ KUMAR LIMBOO          NaN          NaN   \n",
      "20867687  398932.0            ELINA RAI re          NaN          NaN   \n",
      "20867688  398935.0  Husbands  - JUSEIN RAL          NaN          NaN   \n",
      "\n",
      "         Mother_name       House_number  Gender Age  namelen    sp first_name  \\\n",
      "0                NaN                E-1    MALE  42       11  True     TABING   \n",
      "1                NaN                E-1    MALE  39       11  True     TALUNG   \n",
      "2                NaN                E-1  FEMALE  37       10  True      YABUR   \n",
      "4                NaN                E-1  FEMALE  34       11  True      YAKEN   \n",
      "5                NaN                E-1  FEMALE  32       11  True      YAPET   \n",
      "...              ...                ...     ...  ..      ...   ...        ...   \n",
      "20867683         NaN                NaN  FEMALE  19       19  True     DANIAL   \n",
      "20867684         NaN                NaN    MALE  19       12  True      SIMON   \n",
      "20867686         NaN  NCB/02/143(1) con    MALE  21       16  True        RAJ   \n",
      "20867687         NaN                  )  FEMALE  20       12  True      ELINA   \n",
      "20867688         NaN                NIL  FEMALE  37       22  True   Husbands   \n",
      "\n",
      "              last_name  \n",
      "0                  TABO  \n",
      "1                  TABO  \n",
      "2                  TABO  \n",
      "4                 TABOH  \n",
      "5                 TABOH  \n",
      "...                 ...  \n",
      "20867683   SHANKER KAMI  \n",
      "20867684         TAMANG  \n",
      "20867686   KUMAR LIMBOO  \n",
      "20867687         RAI re  \n",
      "20867688   - JUSEIN RAL  \n",
      "\n",
      "[14610390 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "train = train.drop_duplicates().reset_index(drop=True)\n",
    "test = test.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "computational-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = input['Name'].str.lower()\n",
    "gender = input['Gender']\n",
    "vocab = set(' '.join([str(i) for i in names]))\n",
    "vocab.add('SEP')\n",
    "vocab.add('CLS')\n",
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "mysterious-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$', ';', '@', '€', '=', '2', '&', '_', 'r', '1', '»', ':', '™', '9', 'b', 't', 'g', '~', '3', 'w', '’', '#', '‘', 'CLS', '!', '8', 'o', '«', '|', '“', '£', '+', 'f', '[', '6', 'd', '{', 'SEP', '”', '0', 'q', '/', 'y', ')', 'k', 'a', '7', ' ', '¥', '©', \"'\", '<', 'c', '-', '\"', 'e', 'z', '*', '°', 's', '%', '?', 'j', '®', 'p', '¢', '}', 'x', '—', ']', '>', '\\\\', 'h', '§', '.', 'v', '(', '5', 'n', ',', 'u', 'l', '4', 'i', 'm', 'é'}\n",
      "vocab length is  86\n",
      "length of input is  20867689\n"
     ]
    }
   ],
   "source": [
    "print(vocab)\n",
    "print(\"vocab length is \",len_vocab)\n",
    "print (\"length of input is \",len(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "latest-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = dict((c, i) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "natural-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$': 0, ';': 1, '@': 2, '€': 3, '=': 4, '2': 5, '&': 6, '_': 7, 'r': 8, '1': 9, '»': 10, ':': 11, '™': 12, '9': 13, 'b': 14, 't': 15, 'g': 16, '~': 17, '3': 18, 'w': 19, '’': 20, '#': 21, '‘': 22, 'CLS': 23, '!': 24, '8': 25, 'o': 26, '«': 27, '|': 28, '“': 29, '£': 30, '+': 31, 'f': 32, '[': 33, '6': 34, 'd': 35, '{': 36, 'SEP': 37, '”': 38, '0': 39, 'q': 40, '/': 41, 'y': 42, ')': 43, 'k': 44, 'a': 45, '7': 46, ' ': 47, '¥': 48, '©': 49, \"'\": 50, '<': 51, 'c': 52, '-': 53, '\"': 54, 'e': 55, 'z': 56, '*': 57, '°': 58, 's': 59, '%': 60, '?': 61, 'j': 62, '®': 63, 'p': 64, '¢': 65, '}': 66, 'x': 67, '—': 68, ']': 69, '>': 70, '\\\\': 71, 'h': 72, '§': 73, '.': 74, 'v': 75, '(': 76, '5': 77, 'n': 78, ',': 79, 'u': 80, 'l': 81, '4': 82, 'i': 83, 'm': 84, 'é': 85}\n"
     ]
    }
   ],
   "source": [
    "print(char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "orange-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take input upto max and truncate rest\n",
    "#encode to vector space(one hot encoding)\n",
    "#padd 'END' to shorter sequences\n",
    "train_X = []\n",
    "train_Y=[]\n",
    "train_texts= []\n",
    "train_labels=[]\n",
    "trunc_train_name = [str(i).lower()[0:30] for i in train.Name]\n",
    "for i in trunc_train_name:\n",
    "    tmp = [char_index[j] for j in str(i)]\n",
    "    tmp.append(char_index[\"SEP\"])\n",
    "    tmp.insert(0,char_index[\"CLS\"])\n",
    "    train_X.append(tmp)\n",
    "    train_texts.append(i)\n",
    "for i in train.Gender:\n",
    "    train_labels.append(i)\n",
    "    if i == 'MALE':\n",
    "        train_Y.append(1)\n",
    "    else:\n",
    "        train_Y.append(0)\n",
    "train_labels =train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "meaning-comedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14610390,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dental-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_Y = []\n",
    "test_texts=[]\n",
    "test_labels = []\n",
    "trunc_test_name = [str(i).lower()[0:maxlen] for i in test.Name]\n",
    "for i in trunc_test_name:\n",
    "    tmp = [(char_index[j]) for j in str(i)]\n",
    "    tmp.append((char_index[\"SEP\"]))\n",
    "    tmp.insert(0,(char_index[\"CLS\"]))\n",
    "    test_X.append(tmp)\n",
    "    test_texts.append(i)\n",
    "for i in test.Gender:\n",
    "    test_labels.append(i)\n",
    "    if i == 'MALE':\n",
    "        test_Y.append(1)\n",
    "    else:\n",
    "        test_Y.append(0)\n",
    "test_labels =test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-representative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "motivated-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(train_X)\n",
    "train_Y = np.asarray(train_Y)\n",
    "test_X = np.asarray(test_X)\n",
    "test_Y = np.asarray(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "previous-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "capital-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/jalend15/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /Users/jalend15/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/jalend15/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/jalend15/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "innocent-stanford",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-4603396b2df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 )\n\u001b[1;32m   2394\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2395\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2396\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2581\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m#                    ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         tokens_and_encodings = [\n\u001b[0m\u001b[1;32m    419\u001b[0m             self._convert_encoding(\n\u001b[1;32m    420\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         tokens_and_encodings = [\n\u001b[0;32m--> 419\u001b[0;31m             self._convert_encoding(\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"special_tokens_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-surgeon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-metadata",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings,test_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "korean-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-81c07e7bf4b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# training arguments, defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m    )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} \n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=0.1,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics, \n",
    "   )\n",
    "                                         # evaluation dataset\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "flexible-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "model1 = DistilBertForSequenceClassification.from_pretrained('./results_ER_Full/checkpoint-17000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# metrics=trainer.evaluate()\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model1 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "# model1.to(device)\n",
    "# model1.train()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# optim = AdamW(model1.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     for batch in train_loader:\n",
    "#         optim.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "# model1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-cloud",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "solved-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_trainer=Trainer(model)\n",
    "trainer = Trainer(\n",
    "    model=model1,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "#     eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics, \n",
    "   )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "sustained-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=model1\n",
    "\n",
    "# import torch.utils.data as data_utils\n",
    "\n",
    "# indices = torch.arange(10000)\n",
    "# tr_10k = data_utils.Subset(test_dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "amber-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "little-picking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running prediction *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 64\n",
      "/Users/jalend15/opt/miniconda3/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:376: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4688' max='4688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4688/4688 1:22:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "raw_pred, _, _ = trainer.prediction_loop(test_loader, description=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "surface-contrary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running prediction *****\n",
      "  Num examples = 205530\n",
      "  Batch size = 64\n",
      "/Users/jalend15/opt/miniconda3/lib/python3.8/site-packages/transformers/trainer_pt_utils.py:376: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3212' max='3212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3212/3212 53:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "test_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "raw_pred, _, _ = trainer.prediction_loop(test_loader, description=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "verified-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics1(p,z):\n",
    "    pred, labels = p,z\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "suitable-elder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205530\n",
      "[[ 2.8389678 -2.4154549]\n",
      " [ 2.7212958 -2.297977 ]\n",
      " [ 3.0571337 -2.5950747]\n",
      " ...\n",
      " [-1.9216907  1.895895 ]\n",
      " [-1.2466686  1.1756608]\n",
      " [-1.2018938  1.1364172]]\n",
      "{'accuracy': 0.9560842699362624, 'precision': 0.9528814319216443, 'recall': 0.9687850631136045, 'f1': 0.9607674386257736}\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "print(len(y_pred))\n",
    "print(raw_pred)\n",
    "#print(test_Y)\n",
    "results = compute_metrics1(raw_pred,train_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "mechanical-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94024057, 0.96878506])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred= np.argmax(raw_pred,axis=1)\n",
    "matrix = confusion_matrix(train_labels, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "white-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n",
      "{'accuracy': 0.9421666666666667, 'precision': 0.9437417596628795, 'recall': 0.946644055355527, 'f1': 0.9451906795723926}\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "print(y_pred)\n",
    "#print(test_Y)\n",
    "results = compute_metrics1(raw_pred,test_Y)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "compatible-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93718258, 0.94664406])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred= np.argmax(raw_pred,axis=1)\n",
    "matrix = confusion_matrix(test_Y, y_pred)\n",
    "matrix.diagonal()/matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-rolling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-winning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-times",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-terrain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-ethnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-christmas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-inspiration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
